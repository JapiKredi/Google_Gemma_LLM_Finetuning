{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMCHTDXxHKDgqtOYBNOkPaT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JapiKredi/Google_Gemma_LLM_Finetuning/blob/main/Google_Gemma_LLM_Finetuning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Gemma\n",
        "The Gemma Model, a suite of lightweight open-source generative AI (GenAI) models created by Google DeepMind, targets developers and researchers. It was launched alongside Gemini, Google's proprietary generative AI chatbots.\n",
        "\n",
        "Within the Gemma collection, two primary models stand out: Gemma 2B and Gemma 7B. These models are large language models (LLMs) designed for text-to-text decoding, each offering pretrained and instruction-tuned variations. Gemma 2B features a neural network with 2 billion parameters, while Gemma 7B boasts seven billion parameters.\n",
        "\n",
        "Google provides pretrained and instruction-tuned Gemma models optimized for use on laptops and workstations, accessible to developers across multiple platforms. Additionally, Meta's Llama 2 serves as another open-source AI model tailored for laptop use, positioned more as a business tool compared to Gemma. Gemma is commonly preferred for scientific endeavors, while Llama 2 is seen as better suited for general-purpose tasks."
      ],
      "metadata": {
        "id": "NikPXeohPleb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import Libraries"
      ],
      "metadata": {
        "id": "KNBV_1UxPsaO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "huggingface_hub: This library provides access to models, datasets, and other resources shared by the Hugging Face community.\n",
        "\n",
        "transformers: Formerly known as pytorch-transformers or pytorch-pretrained-bert, this library is developed by Hugging Face. It provides state-of-the-art pre-trained models for natural language understanding (NLU) and natural language generation (NLG) tasks.\n",
        "\n",
        "accelerate: Accelerate is a library developed by Hugging Face that simplifies distributed training for deep learning models and provides an easy-to-use interface for distributed computing frameworks.\n",
        "\n",
        "BitsAndBytes: This library provides functions and utilities for working with binary data in Python. It includes functions for performing bitwise operations.\n",
        "\n",
        "trl: The Text Representation Learning (TRL) library is developed by Hugging Face and provides tools and utilities for training and fine-tuning text representations.\n",
        "\n",
        "peft: PEFT (PyTorch Extensible Fine-Tuning) is a library that extends PyTorch for fine-tuning large language models (LLMs) such as GPT and BERT."
      ],
      "metadata": {
        "id": "EaOmPJQ9PxqB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dSOYXLbHMFEV",
        "outputId": "1f23a0de-c6c6-47a0-f880-bac19255e059"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m388.9/388.9 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.0/9.0 MB\u001b[0m \u001b[31m36.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m297.6/297.6 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q -U huggingface_hub\n",
        "!pip install -q -U transformers\n",
        "!pip install -q -U accelerate\n",
        "!pip install -q -U BitsAndBytes\n",
        "%pip install -q trl\n",
        "%pip install -q peft"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ffKs8tyZPqOK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}