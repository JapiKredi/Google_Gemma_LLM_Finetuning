{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN1V1lfQC0mXZjgGUCq9PfA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JapiKredi/Google_Gemma_LLM_Finetuning/blob/main/Google_Gemma_LLM_Finetuning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Gemma\n",
        "The Gemma Model, a suite of lightweight open-source generative AI (GenAI) models created by Google DeepMind, targets developers and researchers. It was launched alongside Gemini, Google's proprietary generative AI chatbots.\n",
        "\n",
        "Within the Gemma collection, two primary models stand out: Gemma 2B and Gemma 7B. These models are large language models (LLMs) designed for text-to-text decoding, each offering pretrained and instruction-tuned variations. Gemma 2B features a neural network with 2 billion parameters, while Gemma 7B boasts seven billion parameters.\n",
        "\n",
        "Google provides pretrained and instruction-tuned Gemma models optimized for use on laptops and workstations, accessible to developers across multiple platforms. Additionally, Meta's Llama 2 serves as another open-source AI model tailored for laptop use, positioned more as a business tool compared to Gemma. Gemma is commonly preferred for scientific endeavors, while Llama 2 is seen as better suited for general-purpose tasks."
      ],
      "metadata": {
        "id": "NikPXeohPleb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import GenAI Libraries"
      ],
      "metadata": {
        "id": "KNBV_1UxPsaO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "huggingface_hub: This library provides access to models, datasets, and other resources shared by the Hugging Face community.\n",
        "\n",
        "transformers: Formerly known as pytorch-transformers or pytorch-pretrained-bert, this library is developed by Hugging Face. It provides state-of-the-art pre-trained models for natural language understanding (NLU) and natural language generation (NLG) tasks.\n",
        "\n",
        "accelerate: Accelerate is a library developed by Hugging Face that simplifies distributed training for deep learning models and provides an easy-to-use interface for distributed computing frameworks.\n",
        "\n",
        "BitsAndBytes: This library provides functions and utilities for working with binary data in Python. It includes functions for performing bitwise operations.\n",
        "\n",
        "trl: The Text Representation Learning (TRL) library is developed by Hugging Face and provides tools and utilities for training and fine-tuning text representations.\n",
        "\n",
        "peft: PEFT (PyTorch Extensible Fine-Tuning) is a library that extends PyTorch for fine-tuning large language models (LLMs) such as GPT and BERT."
      ],
      "metadata": {
        "id": "EaOmPJQ9PxqB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dSOYXLbHMFEV",
        "outputId": "1f23a0de-c6c6-47a0-f880-bac19255e059"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m388.9/388.9 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.0/9.0 MB\u001b[0m \u001b[31m36.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m297.6/297.6 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.8/119.8 MB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m245.2/245.2 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m542.0/542.0 kB\u001b[0m \u001b[31m31.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m102.0/102.0 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m19.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.1/199.1 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q -U huggingface_hub\n",
        "!pip install -q -U transformers\n",
        "!pip install -q -U accelerate\n",
        "!pip install -q -U BitsAndBytes\n",
        "%pip install -q trl\n",
        "%pip install -q peft"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import Python packages\n",
        "\n",
        "Python basic module\n",
        "\n",
        "os: Provides ways to interact with the operating system and its environment variables.\n",
        "torch: PyTorch library for deep learning applications.\n",
        "pandas: Powerful data processing tool, ideal for handling CSV files and other forms of structured data.\n",
        "re : Provides support for working with regular expressions, enabling powerful pattern-based string operations.\n",
        "Transformers module\n",
        "\n",
        "AutoTokenizer: Used to automatically load a pre-trained tokenizer.\n",
        "AutoModelForCausalLM: Used to automatically load pre-trained models for causal language modeling.\n",
        "BitsAndBytesConfig: Configuration class for setting up the Bits and Bytes tokenizer.\n",
        "AutoConfig: Used to automatically load the model's configuration.\n",
        "TrainingArguments: Defines arguments for training setup.\n",
        "Wordcloud module\n",
        "\n",
        "WordCloud : Python library used for generating word clouds, which are visual representations of text data where the size of each word indicates its frequency or importance.\n",
        "STOPWORDS : set of commonly used words that are often excluded from text analysis because they typically do not carry significant meaning or contribute to the understanding of the text.\n",
        "Datasets module\n",
        "\n",
        "Dataset: A class for handling datasets.\n",
        "Peft module\n",
        "\n",
        "LoraConfig : A configuration class for configuring the Lora model.\n",
        "PeftModel: A class that defines the PEFT model.\n",
        "prepare_model_for_kbit_training : A function that prepares a model for k-bit training.\n",
        "get_peft_model : Function to get the PEFT model.\n",
        "trl module\n",
        "\n",
        "SFTTrainer: Trainer class for SFT (Supervised Fine-Tuning) training.\n",
        "IPython.display module\n",
        "\n",
        "Markdown : Used to output text in Markdown format.\n",
        "display : Used to display objects in Jupyter notebooks."
      ],
      "metadata": {
        "id": "9FcXDz6DSyfM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import os\n",
        "import pandas as pd\n",
        "import re\n",
        "import matplotlib.pyplot as plt\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM,BitsAndBytesConfig, AutoConfig, TrainingArguments, pipeline\n",
        "from wordcloud import WordCloud, STOPWORDS\n",
        "from datasets import Dataset\n",
        "from peft import LoraConfig, PeftModel, prepare_model_for_kbit_training, get_peft_model\n",
        "from trl import SFTTrainer\n",
        "from IPython.display import Markdown as md\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ffKs8tyZPqOK",
        "outputId": "ab253ad4-4990-4598-e2c0-bfe69a79708b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:bitsandbytes.cextension:The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AcLJukkcSvOa"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}